{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNFHIcLTEUJR7O2CDXyk0+E",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Prajjwalbhojak/Projects/blob/main/Artice_Comparision.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DxLlpDonDtRc",
        "outputId": "a53fdc47-216d-4322-d2b7-c1e234e21e97"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.7)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from nltk) (4.64.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk) (1.1.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.7/dist-packages (from nltk) (2022.6.2)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk) (7.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bm2TaqURFXwL",
        "outputId": "6499c3e9-f08f-4789-b7bf-7ce6b7781b7b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (3.6.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (5.2.1)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.15.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.7.3)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.21.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Web Scraping the data from Mint website\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from collections import Counter\n",
        "url2 = [\"https://www.livemint.com/technology/tech-news/layoffs-at-google-employees-warned-to-prepare-for-blood-on-the-streets-11660471571812.html\"]\n",
        "\n",
        "with open('Mint.txt', 'w', encoding='utf-8') as outfile:\n",
        "    for url in url2:\n",
        "        website = requests.get(url)\n",
        "        soup = BeautifulSoup(website.content)\n",
        "        text = [''.join(s.findAll(text=True))for s in soup.findAll('p')]\n",
        "        for item in text:\n",
        "            print(item, file=outfile)"
      ],
      "metadata": {
        "id": "T7jJScLZGVUJ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Web scrapin the data from Indian Express Website\n",
        "url1 = [\"https://indianexpress.com/article/technology/tech-news-technology/google-threatens-employees-with-possible-layoffs-blood-on-streets-report-8093052/\"]\n",
        "\n",
        "with open('IndianExpress.txt', 'w', encoding='utf-8') as outfile1:\n",
        "    for url in url1:\n",
        "        website = requests.get(url)\n",
        "        soup = BeautifulSoup(website.content)\n",
        "        text = [''.join(s.findAll(text=True))for s in soup.findAll('p')]\n",
        "        for item in text:\n",
        "            print(item, file=outfile1)"
      ],
      "metadata": {
        "id": "X7YFjxKadvTk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kGRJzQf8HFps",
        "outputId": "7f76b352-00e2-42cc-ed90-f6643aba16ab"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "file_docs = []\n",
        "\n",
        "with open ('/content/IndianExpress.txt') as f: #Open file and tokenize sentences\n",
        "    tokens = sent_tokenize(f.read())\n",
        "    for line in tokens:\n",
        "        file_docs.append(line)\n",
        "\n",
        "print(\"Number of documents:\",len(file_docs))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PkhIJHCrFXzE",
        "outputId": "ebc10ccb-1031-4de4-fc22-d64ea83e2e22"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of documents: 13\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gen_docs = [[w.lower() for w in word_tokenize(text)] #Tokenize words \n",
        "            for text in file_docs]"
      ],
      "metadata": {
        "id": "QfI5rjgKFX5a"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ogMVz5HTKLwM",
        "outputId": "e87fcfea-e5e9-4dd1-83a5-adb1826a470d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (3.6.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.7.3)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.15.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (5.2.1)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.21.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "from gensim import corpora\n",
        "dictionary = gensim.corpora.Dictionary(gen_docs) #creating a dictionary\n",
        "print(dictionary.token2id)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tjBLigwjFX79",
        "outputId": "e71db407-1ec8-4cea-859c-e7825fa4ff26"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'.': 0, 'don': 1, 'employees': 2, 'end': 3, 'executives': 4, 'good': 5, 'google': 6, 'have': 7, 'if': 8, 'layoffs': 9, 'look': 10, 'of': 11, 'possible': 12, 'quarter': 13, 'reportedly': 14, 'results': 15, 't': 16, 'the': 17, 'warned': 18, '’': 19, 'amongst': 20, 'created': 21, 'fear': 22, 'has': 23, 'jobs': 24, 'losing': 25, 'panic': 26, 'that': 27, 'their': 28, 'this': 29, ',': 30, 'a': 31, 'after': 32, 'an': 33, 'and': 34, 'announced': 35, 'be': 36, 'blood': 37, 'by': 38, 'cloud': 39, 'comes': 40, 'company': 41, 'examination': 42, 'freeze': 43, 'general': 44, 'hiring': 45, 'however': 46, 'in': 47, 'insider': 48, 'its': 49, 'july': 50, 'leadership': 51, 'next': 52, 'no': 53, 'on': 54, 'overall': 55, 'productivity': 56, 'reveals': 57, 'reversing': 58, 's': 59, 'sales': 60, 'saying': 61, 'screenshot': 62, 'shows': 63, 'sign': 64, 'streets.': 65, 'there': 66, 'threatened': 67, 'two-week': 68, 'up': 69, 'viewed': 70, 'will': 71, '“': 72, '”': 73, 'about': 74, 'are': 75, 'employee': 76, 'place': 77, 'put': 78, 'reversed': 79, 'they': 80, 'to': 81, 'told': 82, 'which': 83, 'worried': 84, 'yet': 85, 'been': 86, 'belt': 87, 'everyone': 88, 'said': 89, 'talking': 90, 'tightening': 91, 'as': 92, 'ceo': 93, 'consecutive': 94, 'earnings': 95, 'improved': 96, 'pichai': 97, 'reported': 98, 'second': 99, 'sundar': 100, 'weaker-than-expected': 101, 'create': 102, 'culture': 103, 'customer-focused': 104, 'focused': 105, 'is': 106, 'mission-focused': 107, 'more': 108, 'our': 109, 'products': 110, 'bar': 111, 'both': 112, 'can': 113, 'cnbc': 114, 'distractions': 115, 'excellence': 116, 'he': 117, 'how': 118, 'minimize': 119, 'product': 120, 'quoted': 121, 'raise': 122, 'really': 123, 'should': 124, 'think': 125, 'we': 126, 'acknowledged': 127, 'added': 128, 'ahead.': 129, 'challenging': 130, 'concerns': 131, 'environment': 132, 'facing': 133, 'for': 134, 'further': 135, 'headcount': 136, 'it': 137, 'macro': 138, 'needs': 139, 'not': 140, 'real': 141, 'uncertainty': 142, 'where': 143, 'whole': 144, 'with': 145, 'also': 146, 'during': 147, 'efficiency': 148, 'ideas': 149, 'increase': 150, 'initiative': 151, 'launch': 152, 'meeting': 153, 'simplicity': 154, 'sprint': 155, 'through': 156, '–': 157, '‘': 158, '15': 159, 'all': 160, 'announcing': 161, 'august': 162, 'could': 163, 'get': 164, 'help': 165, 'i': 166, 'internal': 167, 'love': 168, 'share': 169, 'survey': 170, 'till': 171, 'would': 172, 'your': 173, 'conditions': 174, 'down': 175, 'economic': 176, 'even': 177, 'giants': 178, 'grapples': 179, 'industry': 180, 'meanwhile': 181, 'meta': 182, 'netflix': 183, 'number': 184, 'slowed': 185, 'such': 186, 'tech': 187, 'twitter': 188, 'uncertain': 189, '12': 190, 'bus': 191, 'celebrations': 192, 'chennai': 193, 'class': 194, 'i-day': 195, 'mtc': 196, 'over': 197, 'returning': 198, 'run': 199, 'student': 200}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "corpus = [dictionary.doc2bow(gen_doc) for gen_doc in gen_docs] #creating a bag of words that contains word id and its frequency in document"
      ],
      "metadata": {
        "id": "Ioa7L6akJRWt"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "6aFHqimwK49A"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf_idf = gensim.models.TfidfModel(corpus)\n",
        "for doc in tf_idf[corpus]:\n",
        "    print([[dictionary[id], np.around(freq, decimals=2)] for id, freq in doc])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lk-g2qciKv7V",
        "outputId": "3acc3c91-ed1a-4e0d-cae3-e5adaabb7669"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['.', 0.01], ['don', 0.23], ['employees', 0.09], ['end', 0.31], ['executives', 0.31], ['good', 0.31], ['google', 0.14], ['have', 0.23], ['if', 0.23], ['layoffs', 0.23], ['look', 0.23], ['of', 0.12], ['possible', 0.23], ['quarter', 0.18], ['reportedly', 0.31], ['results', 0.23], ['t', 0.23], ['the', 0.04], ['warned', 0.31], ['’', 0.18]]\n",
            "[['.', 0.01], ['employees', 0.11], ['amongst', 0.37], ['created', 0.37], ['fear', 0.37], ['has', 0.14], ['jobs', 0.37], ['losing', 0.37], ['panic', 0.37], ['that', 0.09], ['their', 0.27], ['this', 0.27]]\n",
            "[['.', 0.0], ['don', 0.1], ['employees', 0.04], ['google', 0.06], ['if', 0.1], ['look', 0.1], ['of', 0.11], ['quarter', 0.08], ['results', 0.1], ['t', 0.1], ['the', 0.08], ['’', 0.16], ['that', 0.07], ['this', 0.1], [',', 0.08], ['a', 0.11], ['after', 0.08], ['an', 0.06], ['and', 0.16], ['announced', 0.1], ['be', 0.13], ['blood', 0.14], ['by', 0.08], ['cloud', 0.14], ['comes', 0.14], ['company', 0.11], ['examination', 0.14], ['freeze', 0.21], ['general', 0.14], ['hiring', 0.08], ['however', 0.14], ['in', 0.16], ['insider', 0.1], ['its', 0.08], ['july', 0.14], ['leadership', 0.14], ['next', 0.14], ['no', 0.14], ['on', 0.06], ['overall', 0.1], ['productivity', 0.13], ['reveals', 0.14], ['reversing', 0.14], ['s', 0.14], ['sales', 0.28], ['saying', 0.14], ['screenshot', 0.14], ['shows', 0.14], ['sign', 0.14], ['streets.', 0.14], ['there', 0.21], ['threatened', 0.14], ['two-week', 0.1], ['up', 0.14], ['viewed', 0.14], ['will', 0.28], ['“', 0.11], ['”', 0.11]]\n",
            "[['.', 0.01], ['google', 0.12], ['layoffs', 0.19], ['possible', 0.19], ['the', 0.04], ['has', 0.1], ['that', 0.06], ['a', 0.2], ['after', 0.15], ['be', 0.12], ['company', 0.1], ['freeze', 0.19], ['hiring', 0.15], ['in', 0.15], ['insider', 0.19], ['two-week', 0.19], ['about', 0.15], ['are', 0.19], ['employee', 0.19], ['place', 0.27], ['put', 0.27], ['reversed', 0.27], ['they', 0.27], ['to', 0.1], ['told', 0.19], ['which', 0.27], ['worried', 0.27], ['yet', 0.27]]\n",
            "[['.', 0.01], ['the', 0.05], ['has', 0.14], [',', 0.05], ['an', 0.17], ['company', 0.14], ['its', 0.21], ['“', 0.14], ['”', 0.14], ['about', 0.21], ['employee', 0.27], ['been', 0.37], ['belt', 0.37], ['everyone', 0.37], ['said', 0.21], ['talking', 0.37], ['tightening', 0.37]]\n",
            "[['.', 0.01], ['employees', 0.09], ['google', 0.14], ['of', 0.11], ['quarter', 0.17], ['the', 0.04], ['has', 0.11], ['that', 0.07], [',', 0.04], ['be', 0.14], ['company', 0.11], ['its', 0.17], ['productivity', 0.14], ['to', 0.11], ['told', 0.22], ['as', 0.14], ['ceo', 0.3], ['consecutive', 0.3], ['earnings', 0.3], ['improved', 0.3], ['pichai', 0.17], ['reported', 0.3], ['second', 0.3], ['sundar', 0.3], ['weaker-than-expected', 0.3]]\n",
            "[['.', 0.01], ['that', 0.07], [',', 0.08], ['a', 0.11], ['on', 0.13], ['“', 0.11], ['create', 0.28], ['culture', 0.28], ['customer-focused', 0.28], ['focused', 0.28], ['is', 0.21], ['mission-focused', 0.28], ['more', 0.62], ['our', 0.21], ['products', 0.28]]\n",
            "[['.', 0.01], ['the', 0.03], [',', 0.06], ['and', 0.26], ['by', 0.13], ['on', 0.1], ['productivity', 0.1], ['”', 0.08], ['about', 0.13], ['said', 0.13], ['as', 0.1], ['bar', 0.22], ['both', 0.22], ['can', 0.22], ['cnbc', 0.22], ['distractions', 0.22], ['excellence', 0.22], ['he', 0.16], ['how', 0.22], ['minimize', 0.22], ['product', 0.22], ['quoted', 0.22], ['raise', 0.22], ['really', 0.22], ['should', 0.22], ['think', 0.22], ['we', 0.33]]\n",
            "[['.', 0.01], ['have', 0.14], ['the', 0.05], ['that', 0.09], [',', 0.05], ['a', 0.14], ['be', 0.09], ['company', 0.07], ['productivity', 0.09], ['there', 0.14], ['“', 0.14], ['”', 0.14], ['are', 0.14], ['to', 0.07], ['as', 0.09], ['is', 0.27], ['more', 0.14], ['our', 0.14], ['he', 0.27], ['we', 0.14], ['acknowledged', 0.19], ['added', 0.19], ['ahead.', 0.19], ['challenging', 0.19], ['concerns', 0.19], ['environment', 0.19], ['facing', 0.19], ['for', 0.19], ['further', 0.19], ['headcount', 0.19], ['it', 0.19], ['macro', 0.19], ['needs', 0.19], ['not', 0.19], ['real', 0.19], ['uncertainty', 0.19], ['where', 0.19], ['whole', 0.19], ['with', 0.14]]\n",
            "[['.', 0.01], ['employees', 0.08], ['of', 0.19], ['the', 0.15], ['’', 0.14], [',', 0.04], ['an', 0.12], ['announced', 0.18], ['overall', 0.18], ['to', 0.09], ['pichai', 0.14], ['also', 0.25], ['during', 0.25], ['efficiency', 0.25], ['ideas', 0.18], ['increase', 0.25], ['initiative', 0.25], ['launch', 0.25], ['meeting', 0.25], ['simplicity', 0.25], ['sprint', 0.25], ['through', 0.25], ['–', 0.25], ['‘', 0.25]]\n",
            "[['.', 0.01], ['employees', 0.07], ['that', 0.06], ['their', 0.17], [',', 0.07], ['an', 0.11], ['in', 0.14], ['“', 0.09], ['”', 0.09], ['to', 0.09], ['said', 0.14], ['pichai', 0.14], ['ideas', 0.17], ['15', 0.24], ['all', 0.24], ['announcing', 0.24], ['august', 0.24], ['could', 0.24], ['get', 0.24], ['help', 0.24], ['i', 0.24], ['internal', 0.24], ['love', 0.24], ['share', 0.24], ['survey', 0.24], ['till', 0.24], ['would', 0.24], ['your', 0.24]]\n",
            "[['.', 0.01], ['of', 0.08], ['the', 0.03], ['has', 0.08], [',', 0.09], ['a', 0.08], ['and', 0.12], ['hiring', 0.12], ['on', 0.1], ['as', 0.2], ['with', 0.16], ['conditions', 0.21], ['down', 0.21], ['economic', 0.21], ['even', 0.21], ['giants', 0.21], ['grapples', 0.21], ['industry', 0.21], ['meanwhile', 0.21], ['meta', 0.21], ['netflix', 0.21], ['number', 0.21], ['slowed', 0.21], ['such', 0.21], ['tech', 0.43], ['twitter', 0.21], ['uncertain', 0.21]]\n",
            "[['after', 0.17], ['by', 0.17], ['12', 0.29], ['bus', 0.29], ['celebrations', 0.29], ['chennai', 0.29], ['class', 0.29], ['i-day', 0.29], ['mtc', 0.29], ['over', 0.29], ['returning', 0.29], ['run', 0.29], ['student', 0.29]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " # Creating similarity measure object\n",
        " sims = gensim.similarities.Similarity('My_file',tf_idf[corpus],\n",
        "                                        num_features=len(dictionary))"
      ],
      "metadata": {
        "id": "9x6q57o3K3Pc"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file2_docs = [] #Another document to check from\n",
        "\n",
        "with open ('/content/Mint.txt') as f:\n",
        "    tokens = sent_tokenize(f.read())\n",
        "    for line in tokens:\n",
        "        file2_docs.append(line)\n",
        "\n",
        "print(\"Number of documents:\",len(file2_docs))  \n",
        "for line in file2_docs:\n",
        "    query_doc = [w.lower() for w in word_tokenize(line)]\n",
        "    query_doc_bow = dictionary.doc2bow(query_doc) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CAjQLwDXLPqF",
        "outputId": "1071a05e-14ba-4ca6-f61e-ab6f7f84d7ac"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of documents: 28\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query_doc_tf_idf = tf_idf[query_doc_bow] # performing a similarity query against the corpus\n",
        "print('Comparing Result:', sims[query_doc_tf_idf]) # printing (document_number, document_similarity)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8td4k0qrLZxU",
        "outputId": "cdb225ac-aa73-4613-bb64-9a9672c72180"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comparing Result: [0.         0.1228871  0.09017657 0.06952449 0.         0.02606034\n",
            " 0.15626602 0.02967547 0.17225222 0.02198823 0.16840373 0.04682986\n",
            " 0.        ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "sum_of_sims =(np.sum(sims[query_doc_tf_idf], dtype=np.float32))\n",
        "print(sum_of_sims)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l-1KduB9RHFh",
        "outputId": "9dd1330a-95cc-4f24-f46e-73d41bcfe10b"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.90406406\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3dFcNVOSR52v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Another approach using sentence transformers from Hungging face library"
      ],
      "metadata": {
        "id": "UaJsYlMafb5G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install sentence_transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3qm0ua5OTKeS",
        "outputId": "530f54f8-077b-456c-d5df-cf6f90e8575e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: sentence_transformers in /usr/local/lib/python3.7/dist-packages (2.2.2)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (4.21.3)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.7.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (0.9.1)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (0.1.97)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.21.6)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (1.12.1+cu113)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (0.13.1+cu113)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (3.7)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from sentence_transformers) (4.64.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (3.8.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (21.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.12.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (4.1.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.4.0->sentence_transformers) (6.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence_transformers) (3.0.9)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (0.12.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5.0.0,>=4.6.0->sentence_transformers) (2022.6.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->huggingface-hub>=0.4.0->sentence_transformers) (3.8.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from nltk->sentence_transformers) (1.1.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from nltk->sentence_transformers) (7.1.2)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface-hub>=0.4.0->sentence_transformers) (2.10)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sentence_transformers) (3.1.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->sentence_transformers) (7.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "fd1 = open('/content/IndianExpress.txt','r')\n",
        "fd2 = open('/content/Mint.txt','r')"
      ],
      "metadata": {
        "id": "zrTfBmVBUcXz"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file1 = fd1.read()\n",
        "file2 = fd2.read()"
      ],
      "metadata": {
        "id": "4rY96mcaUf1f"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        },
        "id": "wj0JDra6Ul1q",
        "outputId": "5dd249db-5b4c-4a7d-f767-3948fdeb3961"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\n      Earlier this week, Microsoft laid off 200 employees from R&D projects, taking the toll to 1,800 since July. Now, another big tech company Google is reportedly warning its workers to either boost performance or prepare to leave as \"there will be blood on the streets\" if the next quarterly earnings are to come out in 2022 not good.\\n   According to a report by The New York Post, sales leadership has threatened employees with an \"overall examination of sales productivity and productivity in general.\" In a company message viewed by Insider, it said that if next quarter results \"don\\'t look up, there will be blood on the streets.\"\\n   The warning was first reported by Insider. Google has already extended its hiring freeze this month without making an announcement. The development made existing Google employees \"fearful of layoffs\". Now, the tech giant has reportedly warned employees with layoffs if the results are not productive.\\n   In July this year, Google announced to slow down hirings for two weeks to review its headcount needs and decide on future course of action. The freeze, however, was extended till the rest of 2022. According to Alphabet and Google CEO Sundar Pichai, \"it\\'s clear we are facing a challenging macro environment with more uncertainty ahead.\"\\n   Pichai told employees late last month that they must improve productivity due to fierce economic headwinds. Pichai said that he wanted to solicit ideas from his employees on how to get \"better results faster.\" \"There are real concerns that our productivity as a whole is not where it needs to be for the head count we have,\" he was quoted as saying.\\n   Google parent company Alphabet reported weaker-than-expected earnings and revenue for the April-June period (Q2). Revenue growth slowed to 13 per cent from 62 per cent in the same quarter last year. Google is not the only one to slow down the hirings or to lay off employees. LinkedIn, Meta, Oracle, Twitter, Nvidia, Snap, Uber, Spotify, Intel, Microsoft and Salesforce, are among others to lay off employees amidst the economic slowdown worldwide.\\n (With Inputs from IANS)\\nDownload the Mint app and read premium stories\\nLog in to our website to save your bookmarks. It\\'ll just take a moment.\\nYou are just one step away from creating your watchlist!\\nOops! Looks like you have exceeded the limit to bookmark the image. Remove some to bookmark this image.\\nYour session has expired, please login again.\\nYou are now subscribed to our newsletters. In case you can’t find any email from our side, please check the spam folder.\\nThis is a subscriber only feature Subscribe Now to get daily updates on WhatsApp\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "\n",
        "sentences = [file1,file2]\n",
        "model = SentenceTransformer('sentence-transformers/all-mpnet-base-v1')\n",
        "embeddings = model.encode(sentences)\n",
        "np.dot(embeddings[0], embeddings[1], out=None)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4dN0mW02TNZD",
        "outputId": "a1737ba7-e7aa-41c2-fe32-82bcc3e662d8"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8738959"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vSzDCqePYnhC"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}